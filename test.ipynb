{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test script for app\n",
    "\n",
    "This notebook includes all of the test code I ran to try out different query methods, including: \n",
    "- Sentiment analysis for lyrics: use VADER and HuggingFace pipeline, results may be usable for English lyrics, but was not optimal for other languages\n",
    "- TF-IDF to query for top keywords: use [text2text library](https://github.com/artitw/text2text#tf-idf)\n",
    "- Summarize lyrics and mood to use as query: use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth, SpotifyClientCredentials\n",
    "\n",
    "# Get Spotify app credentials\n",
    "client_id = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "redirect_uri = os.getenv(\"SPOTIFY_REDIRECT_URI\")\n",
    "scope = \"user-library-read playlist-read-private user-top-read\"\n",
    "\n",
    "# Authenticate \n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "oauth_manager = SpotifyOAuth(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    redirect_uri=redirect_uri,\n",
    "    scope=scope\n",
    ")\n",
    "\n",
    "sp = spotipy.Spotify(oauth_manager=oauth_manager)\n",
    "sp.current_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch Spotify songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_songs = sp.current_user_top_tracks(limit=50)\n",
    "user_songs = []\n",
    "for song in top_songs['items']:\n",
    "    user_songs.append({\n",
    "        'id': song['id'],\n",
    "        'title': song['name'],\n",
    "        'artist': song['artists'][0]['name'],\n",
    "        'album': song['album']['name'],\n",
    "        'release_date': song['album']['release_date'],\n",
    "        'popularity': song['popularity']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user_songs = pd.DataFrame(user_songs)\n",
    "user_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match songs with lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "gen_client_access_token = os.getenv(\"GENIUS_CLIENT_TOKEN\")\n",
    "genius = lyricsgenius.Genius(gen_client_access_token, sleep_time = 5)\n",
    "list_lyrics = []\n",
    "\n",
    "for i, song in user_songs.iterrows():\n",
    "    title = song['title']\n",
    "    artist = song['artist']\n",
    "    lyrics = genius.search_song(title, artist)\n",
    "    if lyrics:\n",
    "        list_lyrics.append({\n",
    "            'title': title,\n",
    "            'lyrics': lyrics.lyrics\n",
    "        })\n",
    "    else:\n",
    "        lyrics = genius.search_song(title)\n",
    "        if lyrics:\n",
    "            list_lyrics.append({\n",
    "                'title': title,\n",
    "                'lyrics': lyrics.lyrics\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and tokenize lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')   \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')   \n",
    "\n",
    "def clean_lyrics(df, column):\n",
    "    \"\"\"\n",
    "    Cleans the words without importance and fix the format of the  dataframe's column lyrics \n",
    "    Args:\n",
    "        df (DataFrame): df containing song information\n",
    "        column (str): column to clean\n",
    "    Returns:\n",
    "        df (DataFrame): DataFrame containing the cleaned lyrics\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.lower()\n",
    "    # remove section marker\n",
    "    df[column] = df[column].str.replace(r\"(verse\\s?\\d*|chorus|bridge|outro|intro)\", \"\", regex=True)\n",
    "    df[column] = df[column].str.replace(r\"(instrumental|guitar|solo)\", \"\", regex=True) \n",
    "    df[column] = df[column].str.replace(r\"\\[.*?\\]\", \"\", regex=True)\n",
    "    # remove new line\n",
    "    df[column] = df[column].str.replace(r\"\\n\", \". \", regex=True)\n",
    "    # remove special characters\n",
    "    df[column] = df[column].str.replace(r\"[^\\w\\d'\\s.]+\", \"\", regex=True)\n",
    "    df[column] = df[column].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list_lyrics to DataFrame\n",
    "list_lyrics_df = pd.DataFrame(list_lyrics)\n",
    "\n",
    "# Clean and tokenize lyrics\n",
    "list_lyrics_df = clean_lyrics(list_lyrics_df, 'lyrics')\n",
    "list_lyrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "\n",
    "analyzer = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "for text in list_lyrics_df['lyrics']:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    negative.append(scores['neg'])\n",
    "    neutral.append(scores['neu'])\n",
    "    positive.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "\n",
    "list_lyrics_df['negative'] = negative\n",
    "list_lyrics_df['neutral'] = neutral\n",
    "list_lyrics_df['positive'] = positive\n",
    "list_lyrics_df['compound'] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean sentiment score for dataset\n",
    "list_sentiment = list_lyrics_df[['negative', 'neutral', 'positive', 'compound']].mean(axis=1)\n",
    "list_sentiment.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using [this model](tabularisai/multilingual-sentiment-analysis) from HuggingFace:\n",
    "- Better than VADER: \n",
    "  - Less neutral results\n",
    "  - Language support\n",
    "- VADER: \n",
    "  - Excellent sentiment for English\n",
    "  - Tend to lean toward neutral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# def predict_sentiment(text):\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#         sentiment_map = {\n",
    "#             0: \"sombre\",\n",
    "#             1: \"sad\",\n",
    "#             2: \"neutral\",\n",
    "#             3: \"happy\",\n",
    "#             4: \"ecstatic\"\n",
    "#         }\n",
    "#         return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()] \n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    # sentiment_map = {0: \"Somber\", 1: \"Sad\", 2: \"Neutral\", 3: \"Happy\", 4: \"Estactic\"}\n",
    "    pipe = pipeline(task=\"sentiment-analysis\", model=model_name)\n",
    "    sentiments = pipe(texts)\n",
    "    labels = [sentiment['label'] for sentiment in sentiments]\n",
    "    # return [sentiment_map[int(p)] for p in labels]\n",
    "    return labels\n",
    "    \n",
    "# texts = [\n",
    "#     # English\n",
    "#     \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n",
    "#     # Chinese\n",
    "#     \"这家餐厅的菜味道非常棒！\", \"我对他的回答很失望。\", \"天气今天一般。\",\n",
    "#     # Spanish\n",
    "#     \"¡Me encanta cómo quedó la decoración!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo más o menos.\",\n",
    "#     # Arabic\n",
    "#     \"الخدمة في هذا الفندق رائعة جدًا!\", \"لم يعجبني الطعام في هذا المطعم.\", \"كانت الرحلة عادية。\",\n",
    "#     # Ukrainian\n",
    "#     \"Мені дуже сподобалася ця вистава!\", \"Обслуговування було жахливим.\", \"Книга була посередньою。\",\n",
    "#     # Hindi\n",
    "#     \"यह जगह सच में अद्भुत है!\", \"यह अनुभव बहुत खराब था।\", \"फिल्म ठीक-ठाक थी।\",\n",
    "#     # Bengali\n",
    "#     \"এখানকার পরিবেশ অসাধারণ!\", \"সেবার মান একেবারেই খারাপ।\", \"খাবারটা মোটামুটি ছিল।\",\n",
    "#     # Portuguese\n",
    "#     \"Este livro é fantástico! Eu aprendi muitas coisas novas e inspiradoras.\", \n",
    "#     \"Não gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n",
    "#     # Japanese\n",
    "#     \"このレストランの料理は本当に美味しいです！\", \"このホテルのサービスはがっかりしました。\", \"天気はまあまあです。\",\n",
    "#     # Russian\n",
    "#     \"Я в восторге от этого нового гаджета!\", \"Этот сервис оставил у меня только разочарование.\", \"Встреча была обычной, ничего особенного.\",\n",
    "#     # French\n",
    "#     \"J'adore ce restaurant, c'est excellent !\", \"L'attente était trop longue et frustrante.\", \"Le film était moyen, sans plus.\",\n",
    "#     # Turkish\n",
    "#     \"Bu otelin manzarasına bayıldım!\", \"Ürün tam bir hayal kırıklığıydı.\", \"Konser fena değildi, ortalamaydı.\",\n",
    "#     # Italian\n",
    "#     \"Adoro questo posto, è fantastico!\", \"Il servizio clienti è stato pessimo.\", \"La cena era nella media.\",\n",
    "#     # Polish\n",
    "#     \"Uwielbiam tę restaurację, jedzenie jest świetne!\", \"Obsługa klienta była rozczarowująca.\", \"Pogoda jest w porządku, nic szczególnego.\",\n",
    "#     # Tagalog\n",
    "#     \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n",
    "#     # Dutch\n",
    "#     \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon oké, niet bijzonder.\",\n",
    "#     # Malay\n",
    "#     \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n",
    "#     # Korean\n",
    "#     \"이 가게의 케이크는 정말 맛있어요!\", \"서비스가 너무 별로였어요.\", \"날씨가 그저 그렇네요.\",\n",
    "#     # Swiss German\n",
    "#     \"Ich find dä Service i de Beiz mega guet!\", \"Däs Esä het mir nöd gfalle.\", \"D Wätter hüt isch so naja.\",\n",
    "#     # Vietnamese\n",
    "#     \"Tôi thích cách trang trí mới của quán!\", \"Dịch vụ khách hàng thì thất vọng.\", \"Bộ phim này tạm ổn, không gì đặc biệt.\"\n",
    "# ]\n",
    "\n",
    "# for text, sentiment in zip(texts, predict_sentiment(texts)):\n",
    "#     print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for text in list_lyrics_df['lyrics']:\n",
    "    sentiment = (predict_sentiment([text[:512]])[0])\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "list_lyrics_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding for similarity search with [this model](sentence-transformers/distiluse-base-multilingual-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [\"This is an example\", \"Đây là một ví dụ\", \"C'est un exemple\", \"这是一个例子\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentences: {sentence}\")\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        print(str(j) + f\": {cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_embed = []\n",
    "for text in list_lyrics_df['lyrics']:\n",
    "    lyrics_embed.append(model.encode(text))\n",
    "\n",
    "list_lyrics_df['lyrics_embed'] = lyrics_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "all_lyrics = \"\".join(list_lyrics_df['lyrics'])\n",
    "lyrics_tfidf = vectorizer.fit_transform([all_lyrics])\n",
    "query = vectorizer.get_feature_names_out()\n",
    "query.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: https://colab.research.google.com/drive/1RaWj5SqWvyC2SsCTGg8IAVcl9G5hOB50?usp=sharing\n",
    "import text2text as t2t\n",
    "\n",
    "top_kw = t2t.Tfidfer().transform(all_lyrics)\n",
    "\n",
    "kw_query = []\n",
    "for kw in top_kw:\n",
    "    kw_query += list(kw.keys())\n",
    "\n",
    "kw_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spotipy search with query as top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random 50 top keywords\n",
    "kw_query = random.sample(kw_query, 50)\n",
    "search_recs = sp.search(q=\"\".join(kw_query), type=\"track\", limit=30)\n",
    "recs = []\n",
    "for track in search_recs['tracks']['items']:\n",
    "    recs.append({\n",
    "        'name': track['name'],\n",
    "        'artist': track['artists'][0]['name']\n",
    "    })\n",
    "pd.DataFrame(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize lyrics and mood to enhance top words querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer(text, num_sen = 1):\n",
    "    languages = stopwords.fileids() # list of supported languages\n",
    "    stopWords = set(stopwords.words([language for language in languages]))\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in text.split('.'):\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "    \n",
    "    fdict = FreqDist(words) # frequency distribution\n",
    "    \n",
    "    # assign scores to senteces based on word frequencies\n",
    "    sentence_scores = [sum(fdict[word] for word in word_tokenize(sentence) if word in fdict) for sentence in sentences]\n",
    "    sentence_scores = list(enumerate(sentence_scores))\n",
    "    \n",
    "    # sort descending\n",
    "    sorted_sentences = sorted(sentence_scores, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # Randomly select the top `num_sentences` sentences for the summary\n",
    "    random_sentences = random.sample(sorted_sentences[:10], num_sen)\n",
    "\n",
    "    # Sort the randomly selected sentences based on their original order in the text\n",
    "    summary_sentences = sorted(random_sentences, key=lambda x: x[0])\n",
    "\n",
    "    # Create the summary\n",
    "    summary = ' '.join([sentences[i] for i, _ in summary_sentences])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for lyrics in list_lyrics_df['lyrics']:\n",
    "    summary.append(text_summarizer(lyrics))\n",
    "list_lyrics_df['summary'] = summary\n",
    "list_lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood = \"Working out\"\n",
    "lyrics_sample = list_lyrics_df.sample(6)['summary']\n",
    "mood += \".\".join(lyrics_sample)\n",
    "mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_recs = sp.search(q = mood, type=\"track\", limit=30)\n",
    "recs = []\n",
    "for track in search_recs['tracks']['items']:\n",
    "    recs.append({\n",
    "        'name': track['name'],\n",
    "        'artist': track['artists'][0]['name']\n",
    "    })\n",
    "pd.DataFrame(recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
